{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63db85aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "source": [
    "# Titanic\n",
    "---\n",
    "In this notebook, I will implementing the titanic dataset from kaggle, https://www.kaggle.com/competitions/titanic/overview. The main objective for this project is to use various machine learning modeling techniques to predict the probability of survival for the passengers on board. To train the models, data will be retrived from train.csv, while testing will be conducted on test.csv. Final estimation result from the most accurate model will generate to a csv file, called gender_submission.csv.\n",
    "\n",
    "In the dataset, here is the definition for the variables\n",
    "- survival: Survival; 0 = no, 1 = yes\n",
    "- pclass: Ticket class; 1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "- sex: Sex\n",
    "- age: Age in years\n",
    "- sibsp: # of siblings / spouses aboard the Titanic\n",
    "- parch: # of parents / children aboard the Titanic\n",
    "- ticket: Ticket number\n",
    "- fare: Passenger fare\n",
    "- cabin: Cabin number\n",
    "- embarked: Port of Embarkation; C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "In data preprocessing, embarked variable will be one-hot encoded.\n",
    "\n",
    "In modeling, I will using logit, SVM, Random Forest Classifier, XGBoost and Tensorflow Keras for the modeling.\n",
    "\n",
    "## Modeling\n",
    "### Data Preprocessing\n",
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e86167-f118-4d64-98ff-2060868aaa68",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1697844463187,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n# import train and test csv data from the file and set the index as PassengerId\ntrain = pd.read_csv('train.csv', index_col='PassengerId')\ntest = pd.read_csv('test.csv', index_col='PassengerId')\n\n# Concat the train and test set for further imputation\n# For test set, there is a missing feature Survived. To concat successfully, I will add a feature to the test dataset with zeros\nsurv_col = np.zeros(len(test))\n# Insert the surv_col to the first column in test dataset\ntest.insert(loc=0, column='Survived', value = surv_col)\n# Concate train and test data\ndata = pd.concat([train, test])\nprint(data.isna().sum())",
    "outputsMetadata": {
     "0": {
      "height": 251,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age          263\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           1\n",
      "Cabin       1014\n",
      "Embarked       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import train and test csv data from the file and set the index as PassengerId\n",
    "train = pd.read_csv('train.csv', index_col='PassengerId')\n",
    "test = pd.read_csv('test.csv', index_col='PassengerId')\n",
    "\n",
    "# Concat the train and test set for further imputation\n",
    "# For test set, there is a missing feature Survived. To concat successfully, I will add a feature to the test dataset with zeros\n",
    "surv_col = np.zeros(len(test))\n",
    "# Insert the surv_col to the first column in test dataset\n",
    "test.insert(loc=0, column='Survived', value = surv_col)\n",
    "# Concate train and test data\n",
    "data = pd.concat([train, test])\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c61f2f-6f58-4bc5-96f2-2059e1861948",
   "metadata": {},
   "source": [
    "#### Age\n",
    "For feature Age, I will impute the missing value based on the average of the age from different ticket classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00072a40-ffa1-4c7d-a332-44a475f07ad8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1697844463240,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "'''\n# Use Random number generator for different classes\nmean_1 = data.loc[data.Pclass == 1, 'Age'].mean()\nstd_1 = data.loc[data.Pclass == 1, 'Age'].std()\nmean_2 = data.loc[data.Pclass == 2, 'Age'].mean()\nstd_2 = data.loc[data.Pclass == 2, 'Age'].std()\nmean_3 = data.loc[data.Pclass == 3, 'Age'].mean()\nstd_3 = data.loc[data.Pclass == 2, 'Age'].std()\n\n'''\n# Calculate the average age for different Pclass\nage_1 = data.loc[data.Pclass == 1, 'Age'].mean()\nage_2 = data.loc[data.Pclass == 2, 'Age'].mean()\nage_3 = data.loc[data.Pclass == 3, 'Age'].mean()\n\n# Impute the age based on the average of difference ticket class\ndata.loc[data['Pclass'] == 1,'Age'] = data.loc[data['Pclass'] == 1,'Age'].fillna(age_1)\ndata.loc[data['Pclass'] == 2,'Age'] = data.loc[data['Pclass'] == 2,'Age'].fillna(age_2)\ndata.loc[data['Pclass'] == 3,'Age'] = data.loc[data['Pclass'] == 3,'Age'].fillna(age_3)\nprint(data.isna().sum())",
    "outputsMetadata": {
     "0": {
      "height": 251,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           1\n",
      "Cabin       1014\n",
      "Embarked       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use Random number generator for different classes\n",
    "mean_1 = data.loc[data.Pclass == 1, 'Age'].mean()\n",
    "std_1 = data.loc[data.Pclass == 1, 'Age'].std()\n",
    "mean_2 = data.loc[data.Pclass == 2, 'Age'].mean()\n",
    "std_2 = data.loc[data.Pclass == 2, 'Age'].std()\n",
    "mean_3 = data.loc[data.Pclass == 3, 'Age'].mean()\n",
    "std_3 = data.loc[data.Pclass == 2, 'Age'].std()\n",
    "\n",
    "'''\n",
    "# Calculate the average age for different Pclass\n",
    "age_1 = data.loc[data.Pclass == 1, 'Age'].mean()\n",
    "age_2 = data.loc[data.Pclass == 2, 'Age'].mean()\n",
    "age_3 = data.loc[data.Pclass == 3, 'Age'].mean()\n",
    "\n",
    "# Impute the age based on the average of difference ticket class\n",
    "data.loc[data['Pclass'] == 1,'Age'] = data.loc[data['Pclass'] == 1,'Age'].fillna(age_1)\n",
    "data.loc[data['Pclass'] == 2,'Age'] = data.loc[data['Pclass'] == 2,'Age'].fillna(age_2)\n",
    "data.loc[data['Pclass'] == 3,'Age'] = data.loc[data['Pclass'] == 3,'Age'].fillna(age_3)\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328abf0a-b8a6-4d91-83fa-f4fe1d021fe9",
   "metadata": {},
   "source": [
    "#### Embarked\n",
    "Since the missing values for feature Embarked are only two, I will impute these missing values with the mode of Embarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cd81731-1d3c-4622-adaa-4845f5fdeb94",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1697844463296,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Embarked\n# using the value_counts to see which port have the most passengers on board\nprint(data.Embarked.value_counts())\n# fill the na with the mode\ndata.Embarked.fillna(data.Embarked.mode()[0], inplace=True)",
    "outputsMetadata": {
     "0": {
      "height": 95,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embarked\n",
      "S    914\n",
      "C    270\n",
      "Q    123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Embarked\n",
    "# using the value_counts to see which port have the most passengers on board\n",
    "print(data.Embarked.value_counts())\n",
    "# fill the na with the mode\n",
    "data.Embarked.fillna(data.Embarked.mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85623e03-967b-4b7d-b990-cd7d32cc98f2",
   "metadata": {},
   "source": [
    "#### Fare\n",
    "The missing value for Fare is only 1, I will impute this missing value with the average fare price in the corresponding ticket class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1cf1233-71b6-40d2-9793-090cd4f41e96",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1697844463347,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Fare\n# Identify the one whose fare is missing\nprint(data[data.Fare.isna()])\n# Calculate the average fare from Pclass 3\nfare_3 = data.loc[data.Pclass==3, 'Fare'].mean()\n# impute the missing value with fare_3\ndata.loc[data.Pclass==3, 'Fare'] = data.loc[data.Pclass==3, 'Fare'].fillna(fare_3)\n# Final check for missing values in the data\nprint(data.isna().sum())",
    "outputsMetadata": {
     "0": {
      "height": 349,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived  Pclass                Name   Sex   Age  SibSp  Parch  \\\n",
      "PassengerId                                                                   \n",
      "1044              0.0       3  Storey, Mr. Thomas  male  60.5      0      0   \n",
      "\n",
      "            Ticket  Fare Cabin Embarked  \n",
      "PassengerId                              \n",
      "1044          3701   NaN   NaN        S  \n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin       1014\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fare\n",
    "# Identify the one whose fare is missing\n",
    "print(data[data.Fare.isna()])\n",
    "# Calculate the average fare from Pclass 3\n",
    "fare_3 = data.loc[data.Pclass==3, 'Fare'].mean()\n",
    "# impute the missing value with fare_3\n",
    "data.loc[data.Pclass==3, 'Fare'] = data.loc[data.Pclass==3, 'Fare'].fillna(fare_3)\n",
    "# Final check for missing values in the data\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164d834-0626-4387-b2c2-611994fa0c2a",
   "metadata": {},
   "source": [
    "#### One hot Encoding for categorical features\n",
    "In the data, there are two features, Embarked and Sex, which are categorical type. I will use get_dummies function to change the text to 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f14ec1bb-4111-4d61-84f0-122886d350a7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1697844463395,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# One hot encoding\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\nsex = {'male':0, 'female':1}\ndata['Embarked'] = data['Embarked'].replace(ports)\ndata['Sex'] = data['Sex'].replace(sex)\nprint(data.head())\nprint(data.columns)",
    "outputsMetadata": {
     "0": {
      "height": 251,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived  Pclass  \\\n",
      "PassengerId                     \n",
      "1                 0.0       3   \n",
      "2                 1.0       1   \n",
      "3                 1.0       3   \n",
      "4                 1.0       1   \n",
      "5                 0.0       3   \n",
      "\n",
      "                                                          Name  Sex   Age  \\\n",
      "PassengerId                                                                 \n",
      "1                                      Braund, Mr. Owen Harris    0  22.0   \n",
      "2            Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0   \n",
      "3                                       Heikkinen, Miss. Laina    1  26.0   \n",
      "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0   \n",
      "5                                     Allen, Mr. William Henry    0  35.0   \n",
      "\n",
      "             SibSp  Parch            Ticket     Fare Cabin  Embarked  \n",
      "PassengerId                                                           \n",
      "1                1      0         A/5 21171   7.2500   NaN         0  \n",
      "2                1      0          PC 17599  71.2833   C85         1  \n",
      "3                0      0  STON/O2. 3101282   7.9250   NaN         0  \n",
      "4                1      0            113803  53.1000  C123         0  \n",
      "5                0      0            373450   8.0500   NaN         0  \n",
      "Index(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
      "       'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding\n",
    "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "sex = {'male':0, 'female':1}\n",
    "data['Embarked'] = data['Embarked'].replace(ports)\n",
    "data['Sex'] = data['Sex'].replace(sex)\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400fc5f-a2fd-4d0d-a12f-65af0fd2b39d",
   "metadata": {},
   "source": [
    "#### Dropping the unnecessary columns and split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd7df563-1af2-42e2-863f-6c62cbe2e098",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1697844463451,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Drop columns\ndata = data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n# Split the data\ntrain_clean = data.iloc[:len(train), :]\ntest_clean = data.iloc[len(train):, :]\n# Separate X and y\n# For train data\nX_tr = train_clean.drop(['Survived'], axis=1)\ny_tr = train_clean['Survived']\n# For test data\nX_ts = test_clean.drop(['Survived'], axis=1)"
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data = data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "# Split the data\n",
    "train_clean = data.iloc[:len(train), :]\n",
    "test_clean = data.iloc[len(train):, :]\n",
    "# Separate X and y\n",
    "# For train data\n",
    "X_tr = train_clean.drop(['Survived'], axis=1)\n",
    "y_tr = train_clean['Survived']\n",
    "# For test data\n",
    "X_ts = test_clean.drop(['Survived'], axis=1)\n",
    "y_ts = test_clean['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f51cc-bb8d-4b93-9729-d34598cc1c43",
   "metadata": {},
   "source": [
    "#### Scaling the data\n",
    "Since the feature Fare have higher mean and std comparing to other features, I decide to use StandardScaler to scale this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "829015e6-6a05-436d-957e-bc015190f156",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1697844463503,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Scaling data\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n# Scaling for train data\nfare_tr_scaled = ss.fit_transform(X_tr[['Fare']])\nX_tr = X_tr.drop('Fare', axis=1)\nX_tr['Fare_scale'] = fare_tr_scaled\n# Scaling for test data\nfare_ts_scaled = ss.fit_transform(X_ts[['Fare']])\nX_ts = X_ts.drop('Fare', axis=1)\nX_ts['Fare_sclaed'] = fare_ts_scaled"
   },
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "'''\n",
    "# Scaling for train data\n",
    "fare_tr_scaled = ss.fit_transform(X_tr[['Fare']])\n",
    "X_tr = X_tr.drop('Fare', axis=1)\n",
    "X_tr['Fare_scale'] = fare_tr_scaled\n",
    "# Scaling for test data\n",
    "fare_ts_scaled = ss.fit_transform(X_ts[['Fare']])\n",
    "X_ts = X_ts.drop('Fare', axis=1)\n",
    "X_ts['Fare_scale'] = fare_ts_scaled\n",
    "'''\n",
    "X_tr = ss.fit_transform(X_tr)\n",
    "X_ts = ss.transform(X_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305be648-d2d4-4240-9017-aa7d3d70d672",
   "metadata": {},
   "source": [
    "#### Train Test Split for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53652df2-5bd8-4330-b356-d54d622558e2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1697844463555,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# train test split for train data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_tr, y_tr, test_size=0.2, random_state=1111, stratify=y_tr)"
   },
   "outputs": [],
   "source": [
    "# train test split for train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y_tr, test_size=0.2, random_state=1111, stratify=y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac52a9",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ad5a920",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 9871,
    "lastExecutedAt": 1697844473427,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.linear_model import LogisticRegression\n# Hyperparameters that need to conduct grid search\nC_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\npenalty = ['','l2','l1', 'elasticnet']\nsolver = ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga']\n\n# Specify the Logistic Regression\nlr = LogisticRegression(max_iter=10000)\n# Set up GridSearchCV\nsearcher_lr = GridSearchCV(lr, {'C':C_values, 'penalty': penalty, 'solver': solver}, scoring='roc_auc')\nsearcher_lr.fit(X_train, y_train)\n\n# Print the best parameter and best score value\nprint(searcher_lr.best_params_)\nprint(searcher_lr.best_score_)\nprint(searcher_lr.score(X_test, y_test))",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     },
     "1": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Accuracy Score is 0.787709\n",
      "Logistic Regression Classification AUC ROC Score is 0.793017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Specify the Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "# Fit the model using train sets\n",
    "lr.fit(X_train, y_train)\n",
    "# Calculate Accuracy and AUC ROC score\n",
    "lr_acc = lr.score(X_test, y_test)\n",
    "lr_aucroc = roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])\n",
    "# Print the results\n",
    "print('Logistic Regression Classification Accuracy Score is %f' % lr_acc)\n",
    "print('Logistic Regression Classification AUC ROC Score is %f' % lr_aucroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2dc5c1",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49f33d5b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1697844473479,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "'''\n# Import SVC and use the default setting\nfrom sklearn.svm import SVC\nsvm = SVC(probability=True)\nsvm.fit(X_train, y_train)\nprint(svm.score(X_test, y_test))\n'''",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Accuracy Score is 0.782123\n",
      "Support Vector Machine AUC ROC Score is 0.780237\n"
     ]
    }
   ],
   "source": [
    "# Import SVC and use the default setting\n",
    "from sklearn.svm import SVC\n",
    "# Instantiate SVC\n",
    "svm = SVC(probability=True)\n",
    "# Fit the model using train sets\n",
    "svm.fit(X_train, y_train)\n",
    "# Calculate Accuracy and AUC ROC score\n",
    "svc_acc = svm.score(X_test, y_test)\n",
    "svc_aucroc = roc_auc_score(y_test, svm.predict_proba(X_test)[:,1])\n",
    "# Print the results\n",
    "print('Support Vector Machine Accuracy Score is %f' % svc_acc)\n",
    "print('Support Vector Machine AUC ROC Score is %f' % svc_aucroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92c2ca",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6a8e90e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 674,
    "lastExecutedAt": 1697844474205,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.linear_model import SGDClassifier\n# set the random state = 1111 for reproducibility\nlr_classifier = SGDClassifier(random_state=1111)\n\n# Set the parameters wants to search through\nparams = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 'loss':['hinge','log'], 'penalty':['l1','l2']}\n# Instantiate the GridSearchCV object\nsearcher_SGD = GridSearchCV(lr_classifier, params, scoring='roc_auc')\n# fit the model using train sets\nsearcher_SGD.fit(X_train, y_train)\n\n# print the best parameter results\nprint(searcher_SGD.best_params_)\n# print the best accuracy score from the most fit parameter settings\nprint(searcher_SGD.best_score_)\n# print the accuracy score from test sets\nprint(searcher_SGD.score(X_test, y_test))",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent Accuracy Score is 0.754190\n",
      "Stochastic Gradient Descent AUC ROC Score is 0.754941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# set the random state = 1111 for reproducibility\n",
    "lr_classifier = SGDClassifier(loss='log_loss')\n",
    "\n",
    "# fit the model using train sets\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "# Calculate Accuracy and AUC ROC score\n",
    "sgd_acc = lr_classifier.score(X_test, y_test)\n",
    "sgd_aucroc = roc_auc_score(y_test, lr_classifier.predict_proba(X_test)[:,1])\n",
    "# Print the results\n",
    "print('Stochastic Gradient Descent Accuracy Score is %f' % sgd_acc)\n",
    "print('Stochastic Gradient Descent AUC ROC Score is %f' % sgd_aucroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47b9a9",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd2e5789",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1591,
    "lastExecutedAt": 1697844475796,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Instantiate Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=1111)\n\n# Set the parameters\nparams = {'criterion':['gini', 'entropy', 'log_loss'], 'max_depth':[2, 4, 6, 8, 10], 'min_samples_leaf':[0.1, 0.11, 0.12, 0.14, 0.16, 0.18]}\n# Instantiate the GridSearchCV object\nsearcher_dt = GridSearchCV(dt, params, scoring='roc_auc', cv=5, n_jobs=-1)\n# fit the model using train sets\nsearcher_dt.fit(X_train, y_train)\n\n# print the best parameter results\nprint(searcher_dt.best_params_)\n# print the best accuracy score from the most fit parameter settings\nprint(searcher_dt.best_score_)\n# print the accuracy score from test sets\nprint(searcher_dt.score(X_test, y_test))",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy Score is 0.731844\n",
      "Decision Tree Classifier AUC ROC Score is 0.712055\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model using train sets\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Accuracy and AUC ROC score\n",
    "dt_acc = dt.score(X_test, y_test)\n",
    "dt_aucroc = roc_auc_score(y_test, dt.predict_proba(X_test)[:,1])\n",
    "# Print the results\n",
    "print('Decision Tree Classifier Accuracy Score is %f' % dt_acc)\n",
    "print('Decision Tree Classifier AUC ROC Score is %f' % dt_aucroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015f4c1",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "110538e7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 204578,
    "lastExecutedAt": 1697844681795,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.ensemble import RandomForestClassifier\n\n# Instantiate Random Forest Classifier\nrfc = RandomForestClassifier(random_state=1111)\n\n# Set the parameters\nparams = {'n_estimators':[100, 200, 300], 'criterion':\n          ['gini','entropy','log_loss'], 'max_depth':[2,4,6,8],\n          'min_samples_leaf':[0.1, 0.12, 0.14, 0.16, 0.18], 'max_features':['sqrt','log2','']}\nsearcher_rfc = GridSearchCV(rfc, params, scoring='roc_auc', cv=5, n_jobs=-1)\n# fit the model using train sets\nsearcher_rfc.fit(X_train, y_train)\n\n# print the best parameter results\nprint(searcher_rfc.best_params_)\n# print the best accuracy score from the most fit parameter settings\nprint(searcher_rfc.best_score_)\n# print the accuracy score from test sets\nprint(searcher_rfc.score(X_test, y_test))",
    "outputsMetadata": {
     "0": {
      "height": 76,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy Score is 0.793296\n",
      "Random Forest Classifier AUC ROC Score is 0.856126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# fit the model using train sets\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Accuracy and AUC ROC score\n",
    "rfc_acc = rfc.score(X_test, y_test)\n",
    "rfc_aucroc = roc_auc_score(y_test, rfc.predict_proba(X_test)[:,1])\n",
    "# Print the results\n",
    "print('Random Forest Classifier Accuracy Score is %f' % rfc_acc)\n",
    "print('Random Forest Classifier AUC ROC Score is %f' % rfc_aucroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03aacf-5950-4f9b-a3c6-383a34d4a4ef",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e113fd30-4c34-430f-81d0-d5634ecb1e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          name  accuracy   auc_roc\n",
      "0          Logistic Regression  0.787709  0.793017\n",
      "1       Support Vector Machine  0.782123  0.780237\n",
      "2  Stochastic Gradient Descent  0.754190  0.754941\n",
      "3                Decision Tree  0.731844  0.712055\n",
      "4                Random Forest  0.793296  0.856126\n"
     ]
    }
   ],
   "source": [
    "models = ['Logistic Regression', 'Support Vector Machine', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest']\n",
    "accuracy_list = [lr_acc, svc_acc, sgd_acc, dt_acc, rfc_acc]\n",
    "auc_roc_list = [lr_aucroc, svc_aucroc, sgd_aucroc, dt_aucroc, rfc_aucroc]\n",
    "scoring = {'name':models, 'accuracy': accuracy_list, 'auc_roc':auc_roc_list}\n",
    "print(pd.DataFrame.from_dict(scoring))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d249287-2012-492a-b786-7b04a72e9a07",
   "metadata": {},
   "source": [
    "Based on the scoring result, Random Forest classifier provides the best accuracy and auc_roc scoring among all models built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924126b7-646f-4ce7-9446-195af398f9dc",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35dde56a-6079-4eec-a144-2fd41d90fff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 2, 'min_samples_split': 12, 'n_estimators': 1000}\n",
      "0.8412882891756132\n",
      "0.770949720670391\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'auc_roc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print the accuracy score from test sets\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(searcher_rfc\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(auc_roc_score(y_test, searcher_rfc\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:,\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auc_roc_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters\n",
    "params = {\"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 2, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n",
    "searcher_rfc = GridSearchCV(rfc, params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "# fit the model using train sets\n",
    "searcher_rfc.fit(X_train, y_train)\n",
    "\n",
    "# print the best parameter results\n",
    "print(searcher_rfc.best_params_)\n",
    "# print the best accuracy score from the most fit parameter settings\n",
    "print(searcher_rfc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a657fbb6-e277-412d-88d2-12cd06e8ea1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770949720670391\n",
      "0.8416337285902503\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy score from test sets\n",
    "print(searcher_rfc.score(X_test, y_test))\n",
    "print(roc_auc_score(y_test, searcher_rfc.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4166e-7989-4100-b2d7-f245147f8c38",
   "metadata": {},
   "source": [
    "From the grid search results above, the default setting of the random forest classifier provides a better estimate. Thus, default setting RFC will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88894748-43e4-4f33-94ba-abdfcfb8b390",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 136,
    "lastExecutedAt": 1697846144625,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "predict_prob = searcher_rfc.predict_proba(X_ts)[:,1]\npredict = searcher_rfc.predict(X_ts)\nsubmission = pd.read_csv('gender_submission.csv', index_col='PassengerId')\nsubmission['Survived'] = predict_prob\nprint(submission.head())\nsubmission.to_csv('gender_submission.csv')",
    "outputsMetadata": {
     "0": {
      "height": 154,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 1\n",
      "896                 0\n"
     ]
    }
   ],
   "source": [
    "predict = rfc.predict(X_ts)\n",
    "predict_prob = rfc.predict_proba(X_ts)[:,1]\n",
    "submission = pd.read_csv('gender_submission.csv', index_col='PassengerId')\n",
    "submission['Survived'] = predict\n",
    "submission['Survived'] = submission['Survived'].astype('int64')\n",
    "print(submission.head())\n",
    "submission.to_csv('gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e39a746-94cf-4758-87ab-09d0e9fc9be4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "lastExecutedAt": 1697846145931,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "solution = pd.read_csv('solution.csv', index_col = 'PassengerId')\n# accuracy score\nprint(float(np.sum(predict == solution.Survived)/solution['Survived'].shape[0]))\n# f1 score\n#print(f1_score(solution.Survived, predict))\n# auc roc score\nprint(roc_auc_score(solution.Survived, predict_prob))",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7511961722488039\n",
      "0.792261512555824\n"
     ]
    }
   ],
   "source": [
    "solution = pd.read_csv('solution.csv', index_col = 'PassengerId')\n",
    "# accuracy score\n",
    "print(float(np.sum(predict == solution.Survived)/solution['Survived'].shape[0]))\n",
    "# f1 score\n",
    "#print(f1_score(solution.Survived, predict))\n",
    "# auc roc score\n",
    "print(roc_auc_score(solution.Survived, predict_prob))"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
